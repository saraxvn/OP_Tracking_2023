{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f6710-ae00-483d-a8c2-993fb5a48814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import packages ---\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from scipy import spatial\n",
    "from scipy.spatial.distance import cosine\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Define paths and parameters (to be customized by user) ---\n",
    "df_raw_path_prefix = \"PATH_TO_PICKLE_DIRECTORY/\"\n",
    "video_input_path = \"PATH_TO_VIDEO_FILE.mp4\"\n",
    "participant_id = \"PARTICIPANT_ID\"\n",
    "start_point = 2400  # Adjust to where your analysis window starts\n",
    "\n",
    "# --- Occlusion periods (in seconds) ---\n",
    "occlusion = \"yes\"\n",
    "start_time = 4 * 60 + 4\n",
    "end_time = 5 * 60 + 0\n",
    "\n",
    "occlusion_2 = \"yes\"\n",
    "start_time_2 = 6 * 60 + 3\n",
    "end_time_2 = 6 * 60 + 15\n",
    "\n",
    "# --- Get video FPS ---\n",
    "def get_video_fps(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Could not open video file.\")\n",
    "    fps_value = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "    return fps_value\n",
    "\n",
    "fps = get_video_fps(video_input_path)\n",
    "\n",
    "# Optional: Adjust known variable frame rates\n",
    "if 29 < fps < 31:\n",
    "    fps = 30\n",
    "elif 57 < fps < 59:\n",
    "    fps = 58\n",
    "elif 59 < fps < 61:\n",
    "    fps = 60\n",
    "\n",
    "print(f\"Frames per second (fps): {fps}\")\n",
    "\n",
    "# --- Load raw data ---\n",
    "df_raw = pd.read_pickle(os.path.join(df_raw_path_prefix, f\"{participant_id}_df.pkl\"))\n",
    "\n",
    "# --- Rename columns and prepare working copy ---\n",
    "df_raw = df_raw.loc[:len(df_raw), :]\n",
    "df = df_raw.reset_index()\n",
    "df.columns = [\"Pb1\", \"Pb2\", \"Pb3\"]\n",
    "df_copy = df.copy()\n",
    "\n",
    "# --- Create empty DataFrame for cleaned output ---\n",
    "new_df = pd.DataFrame(columns=[\"P1\", \"P2\", \"P3\"], index=range(0, len(df)))\n",
    "\n",
    "# --- Select 10-minute segment starting from `start_point` ---\n",
    "frame_window = int(10 * 60 * fps)\n",
    "df_head = df_copy.loc[start_point:start_point + frame_window]\n",
    "new_df = new_df.loc[start_point:start_point + frame_window]\n",
    "\n",
    "# --- Trim to start at first non-NaN index ---\n",
    "min_index = df_head[[\"Pb1\", \"Pb2\", \"Pb3\"]].dropna().index.min()\n",
    "df_head = df_head.loc[min_index:min_index + frame_window]\n",
    "new_df = new_df.loc[min_index:min_index + frame_window]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ab230-5309-4704-9acb-807bd01237e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc_distance(v1, v2):\n",
    "    zero_indices = (v1 == 0.0) | (v2 == 0.0)\n",
    "    \n",
    "    # Replace zero values with NaN\n",
    "    v1 = np.where(zero_indices, np.nan, v1)\n",
    "    v2 = np.where(zero_indices, np.nan, v2)\n",
    "\n",
    "    # Calculate Euclidean distance\n",
    "    distance = np.sqrt(np.nansum((v1 - v2) ** 2))\n",
    "    \n",
    "    return distance\n",
    "\n",
    "\n",
    "def cosine_2d(v1, v2):\n",
    "    cosine = spatial.distance.cosine\n",
    "    \n",
    "    # Create masks for non-zero elements\n",
    "    mask_v1 = v1 != 0.0\n",
    "    mask_v2 = v2 != 0.0\n",
    "    \n",
    "    # Create masked arrays\n",
    "    #\n",
    "    masked_v1 = np.ma.masked_array(v1, mask=~mask_v1)\n",
    "    masked_v2 = np.ma.masked_array(v2, mask=~mask_v2)\n",
    "    \n",
    "    # Check if there are non-masked elements in both arrays\n",
    "    if masked_v1.mask.all() or masked_v2.mask.all():\n",
    "        # Handle the case where one or both arrays have only masked values\n",
    "        return 0.0\n",
    "    \n",
    "    # Flatten the masked arrays and calculate cosine similarity\n",
    "    similarity = 1 - cosine(masked_v1.flatten(), masked_v2.flatten())\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def is_float(value):\n",
    "    return isinstance(value, float)\n",
    "\n",
    "def is_numpy_array(value):\n",
    "    return isinstance(value, np.ndarray)\n",
    "\n",
    "def make_whateversnan_nan(df_copy, new_df, i):\n",
    "    Pb1 = df_copy.loc[i, 'Pb1']\n",
    "    Pb2 = df_copy.loc[i, 'Pb2']\n",
    "    Pb3 = df_copy.loc[i, 'Pb3']\n",
    "\n",
    "    P1 = new_df.loc[i, 'P1']\n",
    "    P2 = new_df.loc[i, 'P2']\n",
    "    P3 = new_df.loc[i, 'P3']\n",
    "    P_num_new = sum(x is not None for x in [P1, P2, P3])\n",
    "\n",
    "    if P_num_new == 3 and all(map(is_float, [Pb1, Pb2, Pb3])):\n",
    "        print(f\"three nans for i = {i}\")\n",
    "        new_df.loc[i, 'P1'] = np.nan\n",
    "        new_df.loc[i, 'P2'] = np.nan\n",
    "        new_df.loc[i, 'P3'] = np.nan\n",
    "        pass\n",
    "    elif P_num_new == 2 and is_numpy_array(P1) and \\\n",
    "            sum(map(is_float, [Pb1, Pb2, Pb3])) == 2:\n",
    "        print(f\"two nans moved to P2 and P3 for i = {i}\")\n",
    "        new_df.loc[i, 'P2'] = np.nan\n",
    "        new_df.loc[i, 'P3'] = np.nan\n",
    "        pass\n",
    "    elif P_num_new == 3 and is_numpy_array(P1) and is_numpy_array(P2) and \\\n",
    "            sum(map(is_float, [Pb1, Pb2, Pb3])) == 1:\n",
    "        print(f\"our two arrays are already filled, one nan moved to P3 for i = {i}\")\n",
    "        new_df.loc[i, 'P3'] = np.nan\n",
    "        pass\n",
    "    elif P_num_new == 3 and is_float(P3) and \\\n",
    "            ((is_numpy_array(P1) and is_float(P2)) or (is_float(P1) and is_numpy_array(P2))) and \\\n",
    "            sum(map(is_float, [Pb1, Pb2, Pb3])) == 2:\n",
    "        print(f\"one nan moved to P3 for i = {i}\")\n",
    "        new_df.loc[i, 'P3'] = np.nan\n",
    "        pass\n",
    "    \n",
    "\n",
    "def calculate_coses(new_df, df_copy, i, z, P_num_new):\n",
    "    cosim_1 = cosine_2d(new_df.loc[i-z, 'P' + str(P_num_new)], df_copy.loc[i, 'Pb1'])\n",
    "    cosim_2 = cosine_2d(new_df.loc[i-z, 'P' + str(P_num_new)], df_copy.loc[i, 'Pb2'])\n",
    "    cosim_3 = cosine_2d(new_df.loc[i-z, 'P' + str(P_num_new)], df_copy.loc[i, 'Pb3'])\n",
    "\n",
    "    all_coses = [('Pb1', cosim_1), ('Pb2', cosim_2), ('Pb3', cosim_3)] \n",
    "\n",
    "    # Sort the values in descending order and get the top two\n",
    "    sorted_coses = sorted(all_coses, key=lambda x: x[1], reverse=True)[:2]\n",
    "    return all_coses, sorted_coses\n",
    "\n",
    "def calculate_distances(sorted_coses, df_copy, new_df, i, z, P_num_new):\n",
    "    name_1, array_1 = sorted_coses[0][0], df_copy.loc[i, sorted_coses[0][0]]\n",
    "    name_2, array_2 = sorted_coses[1][0], df_copy.loc[i, sorted_coses[1][0]]\n",
    "\n",
    "    distances_1 = euc_distance(array_1, new_df.loc[i - z, 'P' + str(P_num_new)])\n",
    "    distances_2 = euc_distance(array_2, new_df.loc[i - z, 'P' + str(P_num_new)])\n",
    "\n",
    "    sum_distances_1 = np.nansum(distances_1)\n",
    "    sum_distances_2 = np.nansum(distances_2)\n",
    "\n",
    "    return sum_distances_1, sum_distances_2, name_1, array_1, name_2, array_2, distances_1, distances_2\n",
    "\n",
    "def find_person(df_copy, new_df, i, P_num_new, sum_dist_thresh,  z = 1):\n",
    "    if i - z < 0:\n",
    "        return None\n",
    "    #IF ALL THREE SHOULD BE NAN FILL WITH NAN \n",
    "    make_whateversnan_nan(df_copy, new_df, i)\n",
    "    all_coses, sorted_coses = calculate_coses(new_df, df_copy, i, z, P_num_new)\n",
    "    sum_distances_1, sum_distances_2, name_1, array_1, name_2, array_2, distances_1, distances_2 = calculate_distances(sorted_coses, df_copy, new_df, i, z, P_num_new)\n",
    "    print(all_coses) \n",
    "    print(sorted_coses)\n",
    "\n",
    "    # Check the difference between the top two values\n",
    "    difference = sorted_coses[0][1] - sorted_coses[1][1]\n",
    "    \n",
    "    #if (float(sorted_coses[0][1]) > 0.97 and sum_distances_1 < sum_dist_thresh) or\\\n",
    "    if (difference >= 0.25 and float(sorted_coses[0][1]) > 0.94 and sum_distances_1 < sum_dist_thresh):\n",
    "        max_cos = sorted_coses[0][0]\n",
    "        print(\"For P\" + str(P_num_new) + \" max_cos index is \" + max_cos + \", for i = {0}\".format(i))\n",
    "        new_df.loc[i, 'P' + str(P_num_new)] = df_copy.loc[i, max_cos]\n",
    "    #if the difference is too small \n",
    "    elif (float(sorted_coses[0][1]) > 0.89) or\\\n",
    "    (sum_distances_1 < 300 or sum_distances_2 < 300 and float(sorted_coses[0][1]) > .73): \n",
    "        # COMPARE EUCLIDEAN DISTANCES TO MATCH, given the sum of distances is lower than 100\n",
    "        print(\"For P\" + str(P_num_new) + \" In i = {0}, \".format(i) + \"Not high enough max cosine value or difference between the top two values. Comparing distances.\")\n",
    "        # Extract the original arrays for the top two candidates\n",
    "        if sum_distances_1 > sum_dist_thresh and sum_distances_2 > sum_dist_thresh:\n",
    "            print(\"both sum distances are over thresh at \" + str(sum_distances_1) + \" and \" + str(sum_distances_2) + \"so we changed to NaN for i = {0}\".format(i))\n",
    "            new_df.loc[i, 'P' + str(P_num_new)] = np.nan\n",
    "        elif sum_distances_1 < sum_distances_2 and sum_distances_1 < sum_dist_thresh and sum_distances_1 != 0.0:\n",
    "            print(name_1 + \" distance sum is smaller at \" + str(sum_distances_1) + \" compared to \" + str(sum_distances_2))\n",
    "            new_df.loc[i, 'P' + str(P_num_new)] = df_copy.loc[i, name_1]\n",
    "        elif sum_distances_2 < sum_distances_1 and sum_distances_2 < sum_dist_thresh and sum_distances_2 != 0.0:\n",
    "            print(name_2 + \" has a lower distance at \" + str(sum_distances_2) + \" compared to \" + str(sum_distances_1))\n",
    "            new_df.loc[i, 'P' + str(P_num_new)] = df_copy.loc[i, name_2]\n",
    "    else:\n",
    "        print(\"max cosine similarity value was \" + str(sorted_coses[0][1]) + \" distances were \" + str(sum_distances_1) + \" \" + str(sum_distances_2) + \" for i = {0}\".format(i))\n",
    "        new_df.loc[i, 'P' + str(P_num_new)] = np.nan\n",
    "        \n",
    "def control_fakeP1_and_P2_duplicates(i, new_df, z):\n",
    "    # Assuming new_df is accessible here\n",
    "    comparables = [('P1', new_df.loc[i, 'P1']), ('P2', new_df.loc[i, 'P2']), ('P3', new_df.loc[i, 'P3'])]\n",
    "    if np.array_equal(comparables[0][1], comparables[1][1]):\n",
    "        distances_1 = euc_distance(new_df.loc[i, comparables[0][0]], new_df.loc[i-z, comparables[0][0]])\n",
    "        distances_2 = euc_distance(new_df.loc[i, comparables[1][0]], new_df.loc[i-z, comparables[1][0]])\n",
    "        sum_distances_1 = np.nansum(distances_1)\n",
    "        sum_distances_2 = np.nansum(distances_2)\n",
    "        if sum_distances_1 > sum_distances_2:\n",
    "            new_df.loc[i, comparables[0][0]] = np.nan\n",
    "        else:\n",
    "            new_df.loc[i, comparables[1][0]] = np.nan\n",
    "    elif np.array_equal(comparables[0][1], comparables[2][1]): \n",
    "        distances_1 = euc_distance(new_df.loc[i, comparables[0][0]], new_df.loc[i-z, comparables[0][0]])\n",
    "        distances_2 = euc_distance(new_df.loc[i, comparables[2][0]], new_df.loc[i-z, comparables[2][0]])\n",
    "        sum_distances_1 = np.nansum(distances_1)\n",
    "        sum_distances_2 = np.nansum(distances_2)\n",
    "        if sum_distances_1 > sum_distances_2:\n",
    "            new_df.loc[i, comparables[0][0]] = np.nan\n",
    "        else:\n",
    "            new_df.loc[i, comparables[2][0]] = np.nan\n",
    "    elif np.array_equal(comparables[1][1], comparables[2][1]):\n",
    "        distances_1 = euc_distance(new_df.loc[i, comparables[1][0]], new_df.loc[i-z, comparables[1][0]])\n",
    "        distances_2 = euc_distance(new_df.loc[i, comparables[2][0]], new_df.loc[i-z, comparables[2][0]])\n",
    "        sum_distances_1 = np.nansum(distances_1)\n",
    "        sum_distances_2 = np.nansum(distances_2)\n",
    "        if sum_distances_1 > sum_distances_2:\n",
    "            new_df.loc[i, comparables[1][0]] = np.nan\n",
    "        else:\n",
    "            new_df.loc[i, comparables[2][0]] = np.nan\n",
    "\n",
    "\n",
    "def fill_gap(i, new_df, df_copy, z, P_num_new, sum_dist_thresh):\n",
    "    if P_num_new == 3 and all(is_numpy_array(df_copy.loc[i, col]) for col in ['Pb1', 'Pb2', 'Pb3']):\n",
    "        Pb1 = df_copy.loc[i, 'Pb1']\n",
    "        Pb2 = df_copy.loc[i, 'Pb2']\n",
    "        Pb3 = df_copy.loc[i, 'Pb3']\n",
    "        P1 = new_df.loc[i, 'P1']\n",
    "        P2 = new_df.loc[i, 'P2']\n",
    "        P3 = new_df.loc[i, 'P3']\n",
    "        if all(is_numpy_array(new_df.loc[i, cols]) for cols in ['P1', 'P2']) and is_float(new_df.loc[i, 'P3']):\n",
    "            values = [df_copy.loc[i, 'Pb1'], df_copy.loc[i, 'Pb2'], df_copy.loc[i, 'Pb3']]\n",
    "            other_value = next(val for val in values if not np.array_equal(val, new_df.loc[i, 'P1']) and not np.array_equal(val, new_df.loc[i, 'P2']))\n",
    "            new_df.loc[i, 'P3'] = other_value\n",
    "            print(\"all 3 should be there - filled in P3 with the missing value that didn't qualify tracking for i = {0}\".format(i))\n",
    "        elif all(is_numpy_array(new_df.loc[i, cols2]) for cols2 in ['P2', 'P3']) and is_float(new_df.loc[i, 'P1']):\n",
    "            values = [df_copy.loc[i, 'Pb1'], df_copy.loc[i, 'Pb2'], df_copy.loc[i, 'Pb3']]\n",
    "            other_value = next(val for val in values if not np.array_equal(val, new_df.loc[i, 'P2']) and not np.array_equal(val, new_df.loc[i, 'P3']))\n",
    "            new_df.loc[i, 'P1'] = other_value\n",
    "            print(\"all 3 should be there - filled in P1 with the missing value that didn't qualify tracking for i = {0}\".format(i))\n",
    "        elif all(is_numpy_array(new_df.loc[i, cols3]) for cols3 in ['P1', 'P3']) and is_float(new_df.loc[i, 'P2']):\n",
    "            values = [df_copy.loc[i, 'Pb1'], df_copy.loc[i, 'Pb2'], df_copy.loc[i, 'Pb3']]\n",
    "            other_value = next(val for val in values if not np.array_equal(val, new_df.loc[i, 'P1']) and not np.array_equal(val, new_df.loc[i, 'P3']))\n",
    "            new_df.loc[i, 'P2'] = other_value\n",
    "            print(\"all 3 should be there - filled in P2 with the missing value that didn't qualify tracking for i = {0}\".format(i))\n",
    "    #if there should be 2 arrays and theres just one\n",
    "    elif P_num_new == 3 and sum(type(df_copy.loc[i, col]) == float for col in ['Pb1', 'Pb2', 'Pb3']) == 1 and\\\n",
    "    sum(type(new_df.loc[i, col]) == float for col in ['P1', 'P2', 'P3']) == 2:\n",
    "        Pb1 = df_copy.loc[i, 'Pb1']\n",
    "        Pb2 = df_copy.loc[i, 'Pb2']\n",
    "        Pb3 = df_copy.loc[i, 'Pb3']\n",
    "        P1 = new_df.loc[i, 'P1']\n",
    "        P2 = new_df.loc[i, 'P2']\n",
    "        P3 = new_df.loc[i, 'P3']\n",
    "        #FIND ARRAY X FROM DF COPY, THE MISSING ONE\n",
    "        if is_numpy_array(Pb1):\n",
    "            if not np.array_equal(Pb1, P1) and not np.array_equal(Pb1, P2) and not np.array_equal(Pb1, P3):\n",
    "                array_X = Pb1\n",
    "        elif is_numpy_array(Pb2):\n",
    "            if not np.array_equal(Pb2, P1) and not np.array_equal(Pb2, P2) and not np.array_equal(Pb2, P3):\n",
    "                array_X = Pb2\n",
    "        elif is_numpy_array(Pb3):\n",
    "            if not np.array_equal(Pb3, P1) and not np.array_equal(Pb3, P2) and not np.array_equal(Pb3, P3):\n",
    "                array_X = Pb3\n",
    "        #if P1 is already filled\n",
    "                if is_numpy_array(P1):\n",
    "                    distances_1 = euc_distance(array_X, new_df.loc[i-z, 'P2'])\n",
    "                    distances_2 = euc_distance(array_X, new_df.loc[i-z, 'P3'])\n",
    "                    sum_distances_1 = np.nansum(distances_1)\n",
    "                    sum_distances_2 = np.nansum(distances_2)\n",
    "                    if sum_distances_1 > sum_dist_thresh and sum_distances_2 > sum_dist_thresh and\\\n",
    "                    sum_distances_1 != 0.0 and sum_distances_2 != 0.0:\n",
    "                        print(\"sumdistances of both P2 and P3 columns with the missing array were over thresh\")\n",
    "                        P2 = np.nan\n",
    "                        P3 = np.nan\n",
    "                    elif sum_distances_1 < sum_distances_2:\n",
    "                        P2 = array_X\n",
    "                        print(\"filled P2 in with missing array\")\n",
    "                    elif sum_distances_1 > sum_distances_2:\n",
    "                        P3 = array_X\n",
    "                        print(\"filled P3 in with missing array\")\n",
    "                #IF P2 IS ALREADY FILLED\n",
    "                elif is_numpy_array(P2):\n",
    "                    distances_1 = euc_distance(array_X, new_df.loc[i-z, 'P1'])\n",
    "                    distances_2 = euc_distance(array_X, new_df.loc[i-z, 'P3'])\n",
    "                    sum_distances_1 = np.nansum(distances_1)\n",
    "                    sum_distances_2 = np.nansum(distances_2)\n",
    "                    if sum_distances_1 > sum_dist_thresh and sum_distances_2 > sum_dist_thresh and\\\n",
    "                    sum_distances_1 != 0.0 and sum_distances_2 != 0.0:\n",
    "                        print(\"sumdistances of both P1 and P3 columns with the missing array were over thresh\")\n",
    "                        P1 = np.nan\n",
    "                        P3 = np.nan\n",
    "                    elif sum_distances_1 < sum_distances_2:\n",
    "                        P1 = array_X\n",
    "                        print(\"filled P1 in with missing array\")\n",
    "                    elif sum_distances_1 > sum_distances_2:\n",
    "                        P3 = array_X\n",
    "                        print(\"filled P3 in with missing array\")\n",
    "                elif is_numpy_array(P3):\n",
    "                    distances_1 = euc_distance(array_X, new_df.loc[i-z, 'P1'])\n",
    "                    distances_2 = euc_distance(array_X, new_df.loc[i-z, 'P2'])\n",
    "                    sum_distances_1 = np.nansum(distances_1)\n",
    "                    sum_distances_2 = np.nansum(distances_2)\n",
    "                    if sum_distances_1 > sum_dist_thresh and sum_distances_2 > sum_dist_thresh and\\\n",
    "                    sum_distances_1 != 0.0 and sum_distances_2 != 0.0:\n",
    "                        print(\"sumdistances of both P1 and P2 columns with the missing array were over thresh\")\n",
    "                        P1 = np.nan\n",
    "                        P2 = np.nan\n",
    "                    elif sum_distances_1 < sum_distances_2:\n",
    "                        P1 = array_X\n",
    "                        print(\"filled P1 in with missing array\")\n",
    "                    elif sum_distances_1 > sum_distances_2:\n",
    "                        P2 = array_X\n",
    "                        print(\"filled P2 in with missing array\")\n",
    "\n",
    "\n",
    "def track_df(df_copy, new_df):\n",
    "    for i in tqdm(range(1, len(df_copy))):\n",
    "        # find the appropriate z\n",
    "        #\n",
    "        z_per_person = [] \n",
    "        # now we will fill in this list DETERMINING += Z VALUES FOR I-Z\n",
    "        for P_num_new in [1, 2, 3]:\n",
    "            z = 1\n",
    "            while (i - z >= 0) and np.isnan(new_df.loc[i-z, 'P'+ str(P_num_new)]).any():\n",
    "                z += 1\n",
    "            z_per_person.append(z) #after the for this will be z_per_person=[z1, z2, z3]\n",
    "               # Starts basic tests\n",
    "            z = z_per_person[P_num_new -1]\n",
    "            find_person(df_copy, new_df, i, P_num_new, z = z, sum_dist_thresh = 320,)\n",
    "            control_fakeP1_and_P2_duplicates(i, new_df, z)\n",
    "            fill_gap(i, new_df, df_copy, z, P_num_new, sum_dist_thresh = 320)\n",
    "            control_fakeP1_and_P2_duplicates(i, new_df, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a528581",
   "metadata": {},
   "outputs": [],
   "source": [
    "if occlusion_2 == \"yes\":\n",
    "    # --- Calculate frame numbers  corresponding to the start and end times ---\n",
    "    start_frame = int(start_time * fps)\n",
    "    end_frame = int(end_time * fps)\n",
    "    start_frame_2 = int(start_time_2 * fps)\n",
    "    end_frame_2 = int(end_time_2 * fps)\n",
    "\n",
    "    # --- Frame ranges for occlusions ---\n",
    "    occlusion_frame_range = range(start_frame, end_frame + 1)\n",
    "    occlusion_frame_range_2 = range(start_frame_2, end_frame_2 + 1)\n",
    "\n",
    "    # --- Select df_head segments ---\n",
    "    df_head1 = df_head[df_head.index < start_frame]\n",
    "    df_occlusion = df_head[df_head.index.isin(occlusion_frame_range)]\n",
    "    df_head2 = df_head[(df_head.index > end_frame) & (df_head.index < start_frame_2)]\n",
    "    df_occlusion_2 = df_head[df_head.index.isin(occlusion_frame_range_2)]\n",
    "    df_head3 = df_head[df_head.index > end_frame_2]\n",
    "\n",
    "    # --- Select corresponding new_df segments ---\n",
    "    new_df1 = new_df[new_df.index < start_frame]\n",
    "    new_df2 = new_df[(new_df.index > end_frame) & (new_df.index < start_frame_2)]\n",
    "    new_df3 = new_df[new_df.index > end_frame_2]\n",
    "\n",
    "    # --- Copy first non-NaN frame from each segment ---\n",
    "    min_index1 = df_head1[['Pb1', 'Pb2', 'Pb3']].dropna().index.min()\n",
    "    new_df1.loc[min_index1, 'P1'] = df_head1.loc[min_index1, 'Pb1']\n",
    "    new_df1.loc[min_index1, 'P2'] = df_head1.loc[min_index1, 'Pb2']\n",
    "    new_df1.loc[min_index1, 'P3'] = df_head1.loc[min_index1, 'Pb3']\n",
    "    new_df1 = new_df1[new_df1.index >= min_index1]\n",
    "\n",
    "    min_index2 = df_head2[['Pb1', 'Pb2', 'Pb3']].dropna().index.min()\n",
    "    new_df2.loc[min_index2, 'P1'] = df_head2.loc[min_index2, 'Pb1']\n",
    "    new_df2.loc[min_index2, 'P2'] = df_head2.loc[min_index2, 'Pb2']\n",
    "    new_df2.loc[min_index2, 'P3'] = df_head2.loc[min_index2, 'Pb3']\n",
    "    new_df2 = new_df2[new_df2.index >= min_index2]\n",
    "\n",
    "    min_index3 = df_head3[['Pb1', 'Pb2', 'Pb3']].dropna().index.min()\n",
    "    new_df3.loc[min_index3, 'P1'] = df_head3.loc[min_index3, 'Pb1']\n",
    "    new_df3.loc[min_index3, 'P2'] = df_head3.loc[min_index3, 'Pb2']\n",
    "    new_df3.loc[min_index3, 'P3'] = df_head3.loc[min_index3, 'Pb3']\n",
    "    new_df3 = new_df3[new_df3.index >= min_index3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head1 = df_head1.reset_index()\n",
    "df_head1 = df_head1[[\"Pb1\", \"Pb2\", \"Pb3\"]]\n",
    "new_df1 = new_df1.reset_index()\n",
    "new_df1 = new_df1[[\"P1\", \"P2\", \"P3\"]]\n",
    "#run tracking for first part\n",
    "track_df(df_head1, new_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ff354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_index1 = new_df1.index + min_index1\n",
    "\n",
    "# Create a new DataFrame with the same data and the updated index\n",
    "new_df1_a = new_df1.copy()\n",
    "new_df1_a.index = new_index1\n",
    "\n",
    "df_head2 = df_head2.reset_index()\n",
    "df_head2 = df_head2[[\"Pb1\", \"Pb2\", \"Pb3\"]]\n",
    "new_df2 = new_df2.reset_index()\n",
    "new_df2 = new_df2[[\"P1\", \"P2\", \"P3\"]]\n",
    "\n",
    "track_df(df_head2, new_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299cc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index2 = new_df2.index + min_index2\n",
    "\n",
    "# Create a new DataFrame with the same data and the updated index\n",
    "new_df2_a = new_df2.copy()\n",
    "new_df2_a.index = new_index2\n",
    "\n",
    "df_head3 = df_head3.reset_index()\n",
    "df_head3 = df_head3[[\"Pb1\", \"Pb2\", \"Pb3\"]]\n",
    "new_df3 = new_df3.reset_index()\n",
    "new_df3 = new_df3[[\"P1\", \"P2\", \"P3\"]]\n",
    "\n",
    "track_df(df_head3, new_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3b3b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_index3 = new_df3.index + min_index3\n",
    "\n",
    "# Create a new DataFrame with the same data and the updated index\n",
    "new_df3_a = new_df3.copy()\n",
    "new_df3_a.index = (new_index3)\n",
    "#-((min_index1-start_point)*2)\n",
    "\n",
    "# Assuming new_df1 and new_df2 are your DataFrames\n",
    "new_df = pd.concat([new_df1_a, new_df2_a, new_df3_a])\n",
    "\n",
    "# Get the full range of index values\n",
    "index_range = range(new_df.index.min(), new_df.index.max()+1)\n",
    "\n",
    "# Reindex the concatenated DataFrame with the full index range\n",
    "new_df = new_df.reindex(index_range)\n",
    "\n",
    "# Fill NaN values with NaN (optional, as they are already NaN by default)\n",
    "new_df.fillna(np.nan, inplace=True)\n",
    "\n",
    "\n",
    "new_df.to_pickle(\"new_df_10_mins_\"+ppt+\"_latest_occ.pkl\")\n",
    "new_df.to_csv(\"new_df_10_mins_\"+ppt+\"_latest_occ.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16389fb0",
   "metadata": {},
   "source": [
    "df_head4 = df_head4.reset_index()\n",
    "df_head4 = df_head4[[\"Pb1\", \"Pb2\", \"Pb3\"]]\n",
    "new_df4 = new_df4.reset_index()\n",
    "new_df4 = new_df4[[\"P1\", \"P2\", \"P3\"]]\n",
    "\n",
    "track_df(df_head4, new_df4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "297bb1b1",
   "metadata": {},
   "source": [
    "new_index4 = new_df4.index + min_index4\n",
    "\n",
    "# Create a new DataFrame with the same data and the updated index\n",
    "new_df4_a = new_df4.copy()\n",
    "new_df4_a.index = new_index4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549067a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e5e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880180d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2119d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
